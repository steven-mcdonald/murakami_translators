{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> Import ebooks to text files </span>\n",
    "* select books based on format suffix\n",
    "* epub read using ebooklib and BeautifulSoup\n",
    "* docx read using docx package\n",
    "* pdf read using textraxt\n",
    "* each text format book saved as a pickle archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:41:25.535120Z",
     "start_time": "2020-05-31T09:41:25.018587Z"
    }
   },
   "outputs": [],
   "source": [
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os\n",
    "import docx\n",
    "import textract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in epub format files\n",
    "Functions are defined to convert the epub file to html and then to text format. These functions are applied only to epub format files from the ebook folder. The resulting text files are saved as pickles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:41:25.561234Z",
     "start_time": "2020-05-31T09:41:25.553286Z"
    }
   },
   "outputs": [],
   "source": [
    "def epub2thtml(epub_path):\n",
    "    book = epub.read_epub(epub_path)\n",
    "    chapters = []\n",
    "    for item in book.get_items():\n",
    "        if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "            chapters.append(item.get_content())\n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:41:25.575749Z",
     "start_time": "2020-05-31T09:41:25.564480Z"
    }
   },
   "outputs": [],
   "source": [
    "def chap2text(chap):\n",
    "    blacklist = ['[document]','noscript','header','html','meta','head','input','script']\n",
    "    output = ''\n",
    "    soup = BeautifulSoup(chap, 'html.parser')\n",
    "    text = soup.find_all(text=True)\n",
    "    for t in text:\n",
    "        if t.parent.name not in blacklist:\n",
    "            output += '{} '.format(t)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:41:25.594364Z",
     "start_time": "2020-05-31T09:41:25.580358Z"
    }
   },
   "outputs": [],
   "source": [
    "def thtml2ttext(thtml):\n",
    "    Output = []\n",
    "    for html in thtml:\n",
    "        text =  chap2text(html)\n",
    "        Output.append(text)\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:41:25.606462Z",
     "start_time": "2020-05-31T09:41:25.597837Z"
    }
   },
   "outputs": [],
   "source": [
    "def epub2text(epub_path):\n",
    "    chapters = epub2thtml(epub_path)\n",
    "    ttext = thtml2ttext(chapters)\n",
    "    return ttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:41:25.633302Z",
     "start_time": "2020-05-31T09:41:25.612419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HarukiMurakami_ColorlessTsukuruTazaki',\n",
       " 'HarukiMurakami_TheWindUpBirdChronicle',\n",
       " 'HarukiMurakami_AWildSheepChase',\n",
       " 'HarukiMurakami_NorwegianWood',\n",
       " 'HarukiMurakami_KafkaOnTheShore',\n",
       " 'HarukiMurakami_DanceDanceDance']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_dir = '../../../../Documents/murakami/ebooks/'\n",
    "epub_list = os.listdir(book_dir)\n",
    "epub_list = [x.split('.')[0] for x in epub_list if x.split('.')[1] == 'epub']\n",
    "epub_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:41:31.075768Z",
     "start_time": "2020-05-31T09:41:25.647688Z"
    }
   },
   "outputs": [],
   "source": [
    "out_dir = '../../../../Documents/murakami/pkl_raw_books/'\n",
    "for book in epub_list:\n",
    "    epub_full_path = book_dir + book + '.epub'\n",
    "    out_full_path = out_dir + book + '.pkl'\n",
    "    output=epub2text(epub_full_path)\n",
    "    with open(out_full_path, 'wb') as fp:\n",
    "        pickle.dump(output, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in docx format files\n",
    "A function is defined to read in text from docx files  and applied to docx format files from the ebook folder. The resulting files are again stored as pickles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:41:31.088431Z",
     "start_time": "2020-05-31T09:41:31.078129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HarukiMurakami_TheElephantVanishes']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx_list = os.listdir(book_dir)\n",
    "docx_list = [x.split('.')[0] for x in docx_list if x.split('.')[1] == 'docx']\n",
    "docx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:41:31.102098Z",
     "start_time": "2020-05-31T09:41:31.092417Z"
    }
   },
   "outputs": [],
   "source": [
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:41:31.224169Z",
     "start_time": "2020-05-31T09:41:31.106306Z"
    }
   },
   "outputs": [],
   "source": [
    "for book in docx_list:\n",
    "    docx_full_path = book_dir + book + '.docx'\n",
    "    out_full_path = out_dir + book + '.pkl'\n",
    "    output = getText(docx_full_path)\n",
    "    with open(out_full_path, 'wb') as fp:\n",
    "        pickle.dump(output, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:08:06.340743Z",
     "start_time": "2020-05-28T16:08:06.333143Z"
    }
   },
   "source": [
    "## Read in pdf format files\n",
    "Selecting only pdf format files from the ebook folder.\n",
    "They all relate to a single book in this case ad so are grouped together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:41:31.235373Z",
     "start_time": "2020-05-31T09:41:31.226243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nw_b_split_004-021_1page',\n",
       " 'nw_b_split_022-049_1page',\n",
       " 'nw_b_split_050-066_1page',\n",
       " 'nw_b_split_067-080_1page',\n",
       " 'nw_b_split_081-109_1page',\n",
       " 'nw_b_split_110-129_1page']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_list = os.listdir(book_dir)\n",
    "pdf_list = [x.split('.')[0] for x in pdf_list if x.split('.')[1] == 'pdf']\n",
    "pdf_list.sort()\n",
    "pdf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:42:01.841857Z",
     "start_time": "2020-05-31T09:41:31.238471Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in each pdf chunk and form into string\n",
    "texts = str()\n",
    "for pdf_ in pdf_list:\n",
    "    pdf_full_path = book_dir + pdf_ + '.pdf'\n",
    "#     need decode(\"utf-8\") to convert to string\n",
    "#     need split to split into pages and then select alternate pages as each one is read twice\n",
    "    text = textract.process(pdf_full_path, language='eng', method='pdfminer').decode(\"utf-8\").split('\\x0c')[::2]\n",
    "    text_sel = str()\n",
    "    for t in text:\n",
    "        text_sel += t\n",
    "    texts += text_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:42:01.853471Z",
     "start_time": "2020-05-31T09:42:01.844364Z"
    }
   },
   "outputs": [],
   "source": [
    "out_full_path = out_dir + 'HarukiMurakami_NorwegianWoodB' + '.pkl'\n",
    "with open(out_full_path, 'wb') as fp:\n",
    "    pickle.dump(texts, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
