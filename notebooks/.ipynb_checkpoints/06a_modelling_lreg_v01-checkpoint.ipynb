{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> Main Modelling - Logistic Regression - Gridsearch </span>\n",
    "\n",
    "* read in pickle v02\n",
    "* keep 3 translators\n",
    "* try different feature sets and save model each time\n",
    "* compare results at the end\n",
    "* all features available ie normalised counts and pos counts with some drops plus unique words, adj, adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T20:34:41.072542Z",
     "start_time": "2020-06-03T20:34:36.584068Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/Steven/opt/anaconda3/envs/textacy/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textacy\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, average_precision_score, roc_auc_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import scikitplot as skplt\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(sns.color_palette(\"husl\", 3))\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "# extend limit of number of rows and columns to display in cell\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataframe containing text chunks and related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T20:35:07.571984Z",
     "start_time": "2020-06-03T20:34:41.075653Z"
    }
   },
   "outputs": [],
   "source": [
    "in_full_path = '../../../../Documents/murakami/pkl3/df_all_v02.pkl'\n",
    "# read back pickle\n",
    "with open (in_full_path, 'rb') as fp:\n",
    "    df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the dataframe is as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T20:35:07.734341Z",
     "start_time": "2020-06-03T20:35:07.575303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_chunk_no</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>fstop_indices</th>\n",
       "      <th>split_indices</th>\n",
       "      <th>chunks</th>\n",
       "      <th>translator</th>\n",
       "      <th>book_title</th>\n",
       "      <th>parsed</th>\n",
       "      <th>n_sents</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>n_syllables</th>\n",
       "      <th>n_unique_words</th>\n",
       "      <th>n_long_words</th>\n",
       "      <th>n_monosyllable_words</th>\n",
       "      <th>n_polysyllable_words</th>\n",
       "      <th>trans_code</th>\n",
       "      <th>chunk_length</th>\n",
       "      <th>n_sents_norm</th>\n",
       "      <th>n_words_norm</th>\n",
       "      <th>n_chars_norm</th>\n",
       "      <th>n_syllables_norm</th>\n",
       "      <th>n_unique_words_norm</th>\n",
       "      <th>n_long_words_norm</th>\n",
       "      <th>n_monosyllable_words_norm</th>\n",
       "      <th>n_polysyllable_words_norm</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>pron_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>det_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>num_count</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adp_count</th>\n",
       "      <th>cconj_count</th>\n",
       "      <th>sconj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>aux_count</th>\n",
       "      <th>part_count</th>\n",
       "      <th>propn_count</th>\n",
       "      <th>space_count</th>\n",
       "      <th>intj_count</th>\n",
       "      <th>sym_count</th>\n",
       "      <th>x_count</th>\n",
       "      <th>...</th>\n",
       "      <th>then_adv</th>\n",
       "      <th>more_adv</th>\n",
       "      <th>even_adv</th>\n",
       "      <th>why_adv</th>\n",
       "      <th>maybe_adv</th>\n",
       "      <th>again_adv</th>\n",
       "      <th>now_adv</th>\n",
       "      <th>just_adv</th>\n",
       "      <th>how_adv</th>\n",
       "      <th>where_adv</th>\n",
       "      <th>very_adv</th>\n",
       "      <th>only_adv</th>\n",
       "      <th>there_adv</th>\n",
       "      <th>still_adv</th>\n",
       "      <th>so_adv</th>\n",
       "      <th>too_adv</th>\n",
       "      <th>when_adv</th>\n",
       "      <th>all_adv</th>\n",
       "      <th>here_adv</th>\n",
       "      <th>never_adv</th>\n",
       "      <th>as_adv</th>\n",
       "      <th>new_adj</th>\n",
       "      <th>other_adj</th>\n",
       "      <th>more_adj</th>\n",
       "      <th>small_adj</th>\n",
       "      <th>deep_adj</th>\n",
       "      <th>whole_adj</th>\n",
       "      <th>first_adj</th>\n",
       "      <th>bad_adj</th>\n",
       "      <th>little_adj</th>\n",
       "      <th>next_adj</th>\n",
       "      <th>much_adj</th>\n",
       "      <th>own_adj</th>\n",
       "      <th>hard_adj</th>\n",
       "      <th>last_adj</th>\n",
       "      <th>only_adj</th>\n",
       "      <th>big_adj</th>\n",
       "      <th>right_adj</th>\n",
       "      <th>long_adj</th>\n",
       "      <th>old_adj</th>\n",
       "      <th>strange_adj</th>\n",
       "      <th>same_adj</th>\n",
       "      <th>young_adj</th>\n",
       "      <th>sure_adj</th>\n",
       "      <th>able_adj</th>\n",
       "      <th>real_adj</th>\n",
       "      <th>different_adj</th>\n",
       "      <th>good_adj</th>\n",
       "      <th>few_adj</th>\n",
       "      <th>vlong_words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wednesday Afternoon Picnic</td>\n",
       "      <td>IT WAS A  short one-paragraph item in the morn...</td>\n",
       "      <td>[57, 97, 115, 196, 318, 385, 420, 445, 504, 65...</td>\n",
       "      <td>[967, 1924, 2998, 3982, 4935, 5975, 6995, 7961...</td>\n",
       "      <td>IT WAS A short one-paragraph item in the morni...</td>\n",
       "      <td>Alfred Birnbaum</td>\n",
       "      <td>A Wild Sheep Chase</td>\n",
       "      <td>(IT, WAS, A, short, one, -, paragraph, item, i...</td>\n",
       "      <td>15</td>\n",
       "      <td>174</td>\n",
       "      <td>742</td>\n",
       "      <td>240</td>\n",
       "      <td>116</td>\n",
       "      <td>33</td>\n",
       "      <td>128</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>944</td>\n",
       "      <td>15.889831</td>\n",
       "      <td>184.322034</td>\n",
       "      <td>786.016949</td>\n",
       "      <td>254.237288</td>\n",
       "      <td>122.881356</td>\n",
       "      <td>34.957627</td>\n",
       "      <td>135.59322</td>\n",
       "      <td>18.008475</td>\n",
       "      <td>-0.4798</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.064</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_chunk_no number                       title  \\\n",
       "0              0      1  Wednesday Afternoon Picnic   \n",
       "\n",
       "                                                text  \\\n",
       "0  IT WAS A  short one-paragraph item in the morn...   \n",
       "\n",
       "                                       fstop_indices  \\\n",
       "0  [57, 97, 115, 196, 318, 385, 420, 445, 504, 65...   \n",
       "\n",
       "                                       split_indices  \\\n",
       "0  [967, 1924, 2998, 3982, 4935, 5975, 6995, 7961...   \n",
       "\n",
       "                                              chunks       translator  \\\n",
       "0  IT WAS A short one-paragraph item in the morni...  Alfred Birnbaum   \n",
       "\n",
       "           book_title                                             parsed  \\\n",
       "0  A Wild Sheep Chase  (IT, WAS, A, short, one, -, paragraph, item, i...   \n",
       "\n",
       "   n_sents  n_words  n_chars  n_syllables  n_unique_words  n_long_words  \\\n",
       "0       15      174      742          240             116            33   \n",
       "\n",
       "   n_monosyllable_words  n_polysyllable_words  trans_code  chunk_length  \\\n",
       "0                   128                    17           0           944   \n",
       "\n",
       "   n_sents_norm  n_words_norm  n_chars_norm  n_syllables_norm  \\\n",
       "0     15.889831    184.322034    786.016949        254.237288   \n",
       "\n",
       "   n_unique_words_norm  n_long_words_norm  n_monosyllable_words_norm  \\\n",
       "0           122.881356          34.957627                  135.59322   \n",
       "\n",
       "   n_polysyllable_words_norm  vader_compound  vader_neg  vader_neu  vader_pos  \\\n",
       "0                  18.008475         -0.4798      0.075      0.862      0.064   \n",
       "\n",
       "   pron_count  verb_count  det_count  adj_count  num_count  punct_count  \\\n",
       "0        18.0        20.0       31.0        9.0        2.0         33.0   \n",
       "\n",
       "   noun_count  adp_count  cconj_count  sconj_count  adv_count  aux_count  \\\n",
       "0        51.0       19.0          6.0          3.0        5.0        5.0   \n",
       "\n",
       "   part_count  propn_count  space_count  intj_count  sym_count  x_count  ...  \\\n",
       "0         3.0          2.0          0.0         0.0        0.0      0.0  ...   \n",
       "\n",
       "   then_adv  more_adv  even_adv  why_adv  maybe_adv  again_adv  now_adv  \\\n",
       "0         0         0         1        0          0          0        0   \n",
       "\n",
       "   just_adv  how_adv  where_adv  very_adv  only_adv  there_adv  still_adv  \\\n",
       "0         0        0          1         0         0          0          0   \n",
       "\n",
       "   so_adv  too_adv  when_adv  all_adv  here_adv  never_adv  as_adv  new_adj  \\\n",
       "0       0        0         0        0         0          0       0        0   \n",
       "\n",
       "   other_adj  more_adj  small_adj  deep_adj  whole_adj  first_adj  bad_adj  \\\n",
       "0          0         0          0         0          1          0        0   \n",
       "\n",
       "   little_adj  next_adj  much_adj  own_adj  hard_adj  last_adj only_adj  \\\n",
       "0           0         0         0        0         0         0        0   \n",
       "\n",
       "  big_adj right_adj long_adj  old_adj  strange_adj  same_adj  young_adj  \\\n",
       "0       0         0        0        1            0         0          0   \n",
       "\n",
       "   sure_adj  able_adj  real_adj  different_adj  good_adj  few_adj  \\\n",
       "0         0         0         0              0         0        0   \n",
       "\n",
       "   vlong_words_count  \n",
       "0                  0  \n",
       "\n",
       "[1 rows x 142 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T20:35:07.753135Z",
     "start_time": "2020-06-03T20:35:07.742031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5212, 142)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T20:35:07.773047Z",
     "start_time": "2020-06-03T20:35:07.763880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['book_chunk_no', 'number', 'title', 'text', 'fstop_indices',\n",
       "       'split_indices', 'chunks', 'translator', 'book_title', 'parsed',\n",
       "       ...\n",
       "       'strange_adj', 'same_adj', 'young_adj', 'sure_adj', 'able_adj',\n",
       "       'real_adj', 'different_adj', 'good_adj', 'few_adj',\n",
       "       'vlong_words_count'],\n",
       "      dtype='object', length=142)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T20:35:07.800792Z",
     "start_time": "2020-06-03T20:35:07.778532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39927091327705294"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_acc = df['trans_code'].value_counts(normalize=True).max()\n",
    "baseline_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features = Basic Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose features to include in modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T12:18:52.336312Z",
     "start_time": "2020-06-03T12:18:52.324060Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_select(basic_counts=True, vader=False, pos_counts=False, \n",
    "                   words=False, adv=False, adj=False):\n",
    "    '''create column list depending on features to include in the modelling'''\n",
    "    columns = []\n",
    "    if basic_counts:\n",
    "        columns += [i for i in df.columns if i.startswith('n_') & i.endswith('_norm')]\n",
    "    if vader:\n",
    "        columns += [i for i in df.columns if i.startswith('vader_')]\n",
    "    if pos_counts:\n",
    "        columns += [i for i in df.columns if i.endswith('_count_norm')]\n",
    "    if words:\n",
    "        columns += [i for i in df.columns if i.endswith('_w')]\n",
    "    if adj:\n",
    "        columns += [i for i in df.columns if i.endswith('_adj')]\n",
    "    if adv:\n",
    "        columns += [i for i in df.columns if i.endswith('_adv')]\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T12:18:52.682025Z",
     "start_time": "2020-06-03T12:18:52.677859Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_cols = feature_select()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop some columns which may be linked to page formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T12:18:53.348715Z",
     "start_time": "2020-06-03T12:18:53.341805Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = ['num_count_norm', 'punct_count_norm','space_count_norm', 'sym_count_norm', 'x_count_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T12:18:53.713480Z",
     "start_time": "2020-06-03T12:18:53.707380Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_cols = [x for x in predictor_cols if x not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Target, Predictors\n",
    "* set X, y based on selected columns\n",
    "* perform train test split\n",
    "* normalise predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T12:18:56.298107Z",
     "start_time": "2020-06-03T12:18:56.289048Z"
    }
   },
   "outputs": [],
   "source": [
    "def modelling_prep(df, predictor_cols, target_col):\n",
    "#     set predictor and target variables\n",
    "    X = df[predictor_cols]\n",
    "    y = df[target_col]\n",
    "#     perform train test split, including original indices before shuffling\n",
    "    indices = list(df.index)\n",
    "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, indices, test_size=0.2, stratify=y, random_state=1)\n",
    "#     normalise the predictor variables \n",
    "    scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, idx_train, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T12:18:57.301614Z",
     "start_time": "2020-06-03T12:18:57.242389Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, idx_train, idx_test = modelling_prep(df, predictor_cols, 'trans_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Gridsearch Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T12:19:00.201310Z",
     "start_time": "2020-06-03T12:19:00.190432Z"
    }
   },
   "outputs": [],
   "source": [
    "def lr_mc_gridsearch(X_train, y_train):\n",
    "    # set model\n",
    "    model = LogisticRegression(solver='liblinear', multi_class='ovr', max_iter=1000)\n",
    "    # set typical grid search parameters\n",
    "#     params = {'C': np.logspace(-4, 4, 10),\n",
    "#           'penalty': ['l1', 'l2'],\n",
    "#           'fit_intercept': [True, False]}\n",
    "    params = {'C': [0.01, 0.1, 1, 10, 100],\n",
    "              'penalty': ['l1', 'l2']}\n",
    "\n",
    "    # instantiate model\n",
    "    model_gs = GridSearchCV(estimator=model,\n",
    "                  param_grid=params,\n",
    "                  cv=5,\n",
    "                  scoring='accuracy',\n",
    "                  return_train_score=True)\n",
    "    # fit the model\n",
    "    model_gs.fit(X_train, y_train)\n",
    "    return model_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T12:19:03.601135Z",
     "start_time": "2020-06-03T12:19:03.585482Z"
    }
   },
   "outputs": [],
   "source": [
    "def gridsearch_score(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # print the grid search results and store as a dictionary\n",
    "    results_dict = {}\n",
    "    results_dict['Best_Parameters'] = model.best_params_\n",
    "    results_dict['Best_CV_Score'] = model.best_score_\n",
    "    results_dict['Best_Train_Score'] = model.score(X_train, y_train)\n",
    "    results_dict['Best_Test_Score'] = model.score(X_test, y_test)\n",
    "\n",
    "    print('Best Parameters:')\n",
    "    print(results_dict['Best_Parameters'])\n",
    "    print('Best estimator mean cross validated training score:')\n",
    "    print(results_dict['Best_CV_Score'])\n",
    "    print('Best estimator score on the full training set:')\n",
    "    print(results_dict['Best_Train_Score'])\n",
    "    print('Best estimator score on the test set:')\n",
    "    print(results_dict['Best_Test_Score'])\n",
    "    print('ROC-AUC score on the test set:')\n",
    "    \n",
    "    y_bin = label_binarize(y_test, model.classes_)\n",
    "    for i, class_ in enumerate(model.classes_):\n",
    "        print('Class {}:'.format(class_), round(roc_auc_score(y_bin[:,i],model.predict_proba(X_test)[:,i]),2))\n",
    "    results_dict['AUC_Class_0'] = roc_auc_score(y_bin[:,0],model.predict_proba(X_test)[:,0])\n",
    "    results_dict['AUC_Class_1'] = roc_auc_score(y_bin[:,1],model.predict_proba(X_test)[:,1])\n",
    "    results_dict['AUC_Class_2'] = roc_auc_score(y_bin[:,2],model.predict_proba(X_test)[:,2])\n",
    "    predictions = model.predict(X_test)\n",
    "    results_dict['conmat'] = confusion_matrix(\n",
    "        y_test, predictions, labels=[0, 1, 2])\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T12:19:16.267060Z",
     "start_time": "2020-06-03T12:19:04.974351Z"
    }
   },
   "outputs": [],
   "source": [
    "lreg_gs_01 = lr_mc_gridsearch(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T12:19:16.427889Z",
     "start_time": "2020-06-03T12:19:16.332057Z"
    }
   },
   "outputs": [],
   "source": [
    "lreg_gs_01_results = gridsearch_score(lreg_gs_01, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:41:52.610594Z",
     "start_time": "2020-06-02T19:41:52.603248Z"
    }
   },
   "outputs": [],
   "source": [
    "lreg_gs_01_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is a very slight improvement over the basic logistic regression - optimizing parameters with gridsearch does not improve accuracy significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:42:14.116378Z",
     "start_time": "2020-06-02T19:42:14.108308Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the output path and name\n",
    "out_path = '../../../../Documents/murakami/pkl_models/'\n",
    "out_name = 'lreg_gs_01'\n",
    "out_full_path = out_path + out_name + '.pkl'\n",
    "\n",
    "# save pickle\n",
    "with open(out_full_path, 'wb') as fp:\n",
    "    pickle.dump(lreg_gs_01, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features = Basic Counts + POS Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:43:15.914093Z",
     "start_time": "2020-06-02T19:43:15.910098Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_cols = feature_select(basic_counts=True, pos_counts=True)\n",
    "predictor_cols = [x for x in predictor_cols if x not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:43:17.355103Z",
     "start_time": "2020-06-02T19:43:17.333430Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, idx_train, idx_test = modelling_prep(df, predictor_cols, 'trans_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:43:33.903805Z",
     "start_time": "2020-06-02T19:43:25.107935Z"
    }
   },
   "outputs": [],
   "source": [
    "lreg_gs_02 = lr_mc_gridsearch(X_train, y_train)\n",
    "lreg_gs_02_results = lr_gridsearch_score(lreg_gs_02, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:44:37.716579Z",
     "start_time": "2020-06-02T19:44:37.709425Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the output path and name\n",
    "out_path = '../../../../Documents/murakami/pkl_models/'\n",
    "out_name = 'lreg_gs_02'\n",
    "out_full_path = out_path + out_name + '.pkl'\n",
    "\n",
    "# save pickle\n",
    "with open(out_full_path, 'wb') as fp:\n",
    "    pickle.dump(lreg_gs_02, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features = Basic Counts + POS Counts + words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:45:57.079637Z",
     "start_time": "2020-06-02T19:45:57.074309Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_cols = feature_select(basic_counts=True, pos_counts=True, words=True)\n",
    "predictor_cols = [x for x in predictor_cols if x not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:46:13.049815Z",
     "start_time": "2020-06-02T19:46:13.026434Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, idx_train, idx_test = modelling_prep(df, predictor_cols, 'trans_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:46:31.356025Z",
     "start_time": "2020-06-02T19:46:18.657722Z"
    }
   },
   "outputs": [],
   "source": [
    "lreg_gs_03 = lr_mc_gridsearch(X_train, y_train)\n",
    "lreg_gs_03_results = lr_gridsearch_score(lreg_gs_03, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:46:47.086080Z",
     "start_time": "2020-06-02T19:46:47.080719Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the output path and name\n",
    "out_path = '../../../../Documents/murakami/pkl_models/'\n",
    "out_name = 'lreg_gs_03'\n",
    "out_full_path = out_path + out_name + '.pkl'\n",
    "\n",
    "# save pickle\n",
    "with open(out_full_path, 'wb') as fp:\n",
    "    pickle.dump(lreg_gs_03, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features = Basic Counts + POS Counts + words + adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:47:12.606553Z",
     "start_time": "2020-06-02T19:47:12.602077Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_cols = feature_select(basic_counts=True, pos_counts=True, words=True, adj=True)\n",
    "predictor_cols = [x for x in predictor_cols if x not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:47:43.698451Z",
     "start_time": "2020-06-02T19:47:43.662229Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, idx_train, idx_test = modelling_prep(df, predictor_cols, 'trans_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:48:21.799846Z",
     "start_time": "2020-06-02T19:47:58.827397Z"
    }
   },
   "outputs": [],
   "source": [
    "lreg_gs_04 = lr_mc_gridsearch(X_train, y_train)\n",
    "lreg_gs_04_results = lr_gridsearch_score(lreg_gs_04, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:48:44.568517Z",
     "start_time": "2020-06-02T19:48:44.562720Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the output path and name\n",
    "out_path = '../../../../Documents/murakami/pkl_models/'\n",
    "out_name = 'lreg_gs_04'\n",
    "out_full_path = out_path + out_name + '.pkl'\n",
    "\n",
    "# save pickle\n",
    "with open(out_full_path, 'wb') as fp:\n",
    "    pickle.dump(lreg_gs_04, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features = Basic Counts + POS Counts + words + adj + adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:49:05.628588Z",
     "start_time": "2020-06-02T19:49:05.623885Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_cols = feature_select(basic_counts=True, pos_counts=True, words=True, adj=True, adv=True)\n",
    "predictor_cols = [x for x in predictor_cols if x not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:51:16.742477Z",
     "start_time": "2020-06-02T19:51:16.699374Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, idx_train, idx_test = modelling_prep(df, predictor_cols, 'trans_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:52:29.367820Z",
     "start_time": "2020-06-02T19:51:29.053666Z"
    }
   },
   "outputs": [],
   "source": [
    "lreg_gs_05 = lr_mc_gridsearch(X_train, y_train)\n",
    "lreg_gs_05_results = lr_gridsearch_score(lreg_gs_05, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:52:58.422680Z",
     "start_time": "2020-06-02T19:52:58.414940Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the output path and name\n",
    "out_path = '../../../../Documents/murakami/pkl_models/'\n",
    "out_name = 'lreg_gs_05'\n",
    "out_full_path = out_path + out_name + '.pkl'\n",
    "\n",
    "# save pickle\n",
    "with open(out_full_path, 'wb') as fp:\n",
    "    pickle.dump(lreg_gs_05, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features = Basic Counts + POS Counts + words + adj + adv + vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:53:27.729392Z",
     "start_time": "2020-06-02T19:53:27.722965Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_cols = feature_select(basic_counts=True, pos_counts=True, vader=True, words=True, adj=True, adv=True)\n",
    "predictor_cols = [x for x in predictor_cols if x not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:53:45.857220Z",
     "start_time": "2020-06-02T19:53:45.816407Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, idx_train, idx_test = modelling_prep(df, predictor_cols, 'trans_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:57:10.284200Z",
     "start_time": "2020-06-02T19:54:04.120417Z"
    }
   },
   "outputs": [],
   "source": [
    "lreg_gs_06 = lr_mc_gridsearch(X_train, y_train)\n",
    "lreg_gs_06_results = lr_gridsearch_score(lreg_gs_06, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:58:01.873422Z",
     "start_time": "2020-06-02T19:58:01.862613Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the output path and name\n",
    "out_path = '../../../../Documents/murakami/pkl_models/'\n",
    "out_name = 'lreg_gs_06'\n",
    "out_full_path = out_path + out_name + '.pkl'\n",
    "\n",
    "# save pickle\n",
    "with open(out_full_path, 'wb') as fp:\n",
    "    pickle.dump(lreg_gs_06, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:58:14.137379Z",
     "start_time": "2020-06-02T19:58:14.132747Z"
    }
   },
   "outputs": [],
   "source": [
    "lreg_results = [lreg_gs_01_results,\n",
    "                lreg_gs_02_results,\n",
    "                lreg_gs_03_results,\n",
    "                lreg_gs_04_results,\n",
    "                lreg_gs_05_results,\n",
    "                lreg_gs_06_results,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:58:25.877950Z",
     "start_time": "2020-06-02T19:58:25.825362Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, results in enumerate(lreg_results):\n",
    "    print(i+1)\n",
    "    print(pd.DataFrame(results['conmat'], index=['actual birnbaum', 'actual rubin', 'actual gabriel'],\n",
    "                             columns=['predicted birnbaum', 'predicted rubin', 'predicted gabriel']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Comments\n",
    "* Model 1 - basic counts: does very well for Birnbaum and Rubin but very badly for Gabriel\n",
    "* Model 2 - basic + POS counts: slight improvement for Birnbaum, a slight drop for Rubin. Gabriel is much improved but still the lowest accuracy\n",
    "* Model 3 - basic + POS + word counts: Gabriel accuracy imrpoves significantly\n",
    "* Model 4 - basic + POS + word counts + adj: no significant improvement on the test scores\n",
    "* Model 5 - basic + POS + word counts + adj + adv: slight improvement across the board\n",
    "* Model 6 - basic + POS + word counts + adj + adv + vader: no significant improvement on the test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report\n",
    "!!! to be set up!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T08:41:52.072851Z",
     "start_time": "2020-05-20T08:41:52.066395Z"
    }
   },
   "outputs": [],
   "source": [
    "# predictions = lreg_gs_01.predict(X_test)\n",
    "# print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:58:44.477022Z",
     "start_time": "2020-06-02T19:58:44.464324Z"
    }
   },
   "outputs": [],
   "source": [
    "model_nums = []\n",
    "cv_scores = []\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "auc_0 = []\n",
    "auc_1 = []\n",
    "auc_2 = []\n",
    "\n",
    "\n",
    "for i, results in enumerate(lreg_results):\n",
    "    model_nums.append(i+1)\n",
    "    cv_scores.append(results['Best_CV_Score'])\n",
    "    train_scores.append(results['Best_Train_Score'])\n",
    "    test_scores.append(results['Best_Test_Score'])\n",
    "    auc_0.append(results['AUC_Class_0'])\n",
    "    auc_1.append(results['AUC_Class_1'])\n",
    "    auc_2.append(results['AUC_Class_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T19:58:46.902569Z",
     "start_time": "2020-06-02T19:58:45.634887Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_palette(['gray', 'tomato', 'darkred', 'black'])\n",
    "dict_cv_scores = {'model': model_nums, \n",
    "                  'cv_scores': cv_scores, \n",
    "                  'train_scores': train_scores,\n",
    "                  'test_scores': test_scores,\n",
    "                  'auc_0': auc_0,\n",
    "                  'auc_1': auc_1,\n",
    "                  'auc_2': auc_2,}\n",
    "df_cv_scores = pd.DataFrame(dict_cv_scores)\n",
    "df_cv_scores['baseline'] = baseline_acc\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, figsize=(12,12))\n",
    "\n",
    "\n",
    "df_cv_scores.sort_values(by='model').plot(x='model', y='cv_scores', linestyle='--', marker='o', ax=ax[0])\n",
    "df_cv_scores.sort_values(by='model').plot(x='model', y='train_scores', linestyle='--', marker='o', ax=ax[0])\n",
    "df_cv_scores.sort_values(by='model').plot(x='model', y='test_scores', linestyle='--', marker='o', ax=ax[0])\n",
    "df_cv_scores.sort_values(by='model').plot(x='model', y='baseline', linestyle='--', marker='o', ax=ax[0])\n",
    "\n",
    "df_cv_scores.sort_values(by='model').plot(x='model', y='auc_0', linestyle='--', marker='o', ax=ax[1])\n",
    "df_cv_scores.sort_values(by='model').plot(x='model', y='auc_1', linestyle='--', marker='o', ax=ax[1])\n",
    "df_cv_scores.sort_values(by='model').plot(x='model', y='auc_2', linestyle='--', marker='o', ax=ax[1])\n",
    "\n",
    "labels = ['', 'basic_counts', 'POS_counts', 'sel_words', 'sel_adj', 'sel_adv', 'sentiment']\n",
    "# labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "# labels[1] = 'Testing'\n",
    "\n",
    "ax[0].set_xticklabels(labels)\n",
    "ax[0].tick_params(axis='x', labelrotation=45)\n",
    "ax[1].set_xticklabels(labels)\n",
    "ax[1].tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig('../../../../Documents/murakami/plots/lreg_sel_feature_scores.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a best logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:03:45.358352Z",
     "start_time": "2020-06-02T20:03:45.348384Z"
    }
   },
   "outputs": [],
   "source": [
    "lreg_gs_06.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:03:49.428458Z",
     "start_time": "2020-06-02T20:03:46.747659Z"
    }
   },
   "outputs": [],
   "source": [
    "predictor_cols = feature_select(basic_counts=True, pos_counts=True, vader=False, words=True, adj=True, adv=True)\n",
    "predictor_cols = [x for x in predictor_cols if x not in cols_to_drop]\n",
    "\n",
    "X = df[predictor_cols]\n",
    "y = df['trans_code']\n",
    "\n",
    "indices = list(df.index)\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, indices, test_size=0.2, stratify=y, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
    "\n",
    "lreg_final = LogisticRegression(C=1, penalty='l1', solver='liblinear', multi_class='ovr', max_iter=1000)\n",
    "\n",
    "lreg_final.fit(X_train, y_train)\n",
    "predictions = lreg_final.predict(X_test)\n",
    "lreg_final.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T08:42:13.993488Z",
     "start_time": "2020-05-20T08:42:13.982536Z"
    }
   },
   "outputs": [],
   "source": [
    "# code could be useful for refactoring some of the code above - extracting from tuples/dicts etc to df\n",
    "# df_pred = pd.DataFrame([(x.r_ui, x.est) for x in predictions_full],\n",
    "#                        columns=['Rating', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:03:54.553313Z",
     "start_time": "2020-06-02T20:03:54.540305Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the output path and name\n",
    "out_path = '../../../../Documents/murakami/pkl_models/'\n",
    "out_name = 'lreg_final'\n",
    "out_full_path = out_path + out_name + '.pkl'\n",
    "\n",
    "# save pickle\n",
    "with open(out_full_path, 'wb') as fp:\n",
    "    pickle.dump(lreg_final, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:05:24.901381Z",
     "start_time": "2020-06-02T20:05:24.893008Z"
    }
   },
   "outputs": [],
   "source": [
    "probabilities = lreg_final.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:05:25.834763Z",
     "start_time": "2020-06-02T20:05:25.823690Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_f1_lines(figsize=(8,6),fontsize=16):\n",
    "    '''Create f1-score level lines to be added to the precison-recall plot'''\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # add lines of constant F1 scores\n",
    "    \n",
    "    for const in np.linspace(0.2,0.9,8):\n",
    "        x_vals = np.linspace(0.001, 0.999, 100)\n",
    "        y_vals = 1./(2./const-1./x_vals)\n",
    "        ax.plot(x_vals[y_vals > 0], y_vals[y_vals > 0],\n",
    "                 color='lightblue', ls='--', alpha=0.9)\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.annotate('f1={0:0.1f}'.format(const),\n",
    "                     xy=(x_vals[-10], y_vals[-2]+0.0), fontsize=fontsize)\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:05:27.733372Z",
     "start_time": "2020-06-02T20:05:27.097923Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_f1_lines()\n",
    "skplt.metrics.plot_precision_recall(y_test, probabilities, \n",
    "                       plot_micro=True, \n",
    "                       title_fontsize=20, text_fontsize=16, cmap=cmap, ax=ax)\n",
    "ax.legend(loc=[1.1,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:05:36.030283Z",
     "start_time": "2020-06-02T20:05:36.025870Z"
    }
   },
   "outputs": [],
   "source": [
    "# label binarizer - not sure if needed?\n",
    "y_bin = label_binarize(y_test, lreg_final.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:05:36.785697Z",
     "start_time": "2020-06-02T20:05:36.759183Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Area under precision-recall curve:')\n",
    "for i, class_ in enumerate(lreg_final.classes_):\n",
    "    print('Class {}:'.format(class_), round(average_precision_score(y_bin[:,i],lreg_final.predict_proba(X_test)[:,i]),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:05:39.093564Z",
     "start_time": "2020-06-02T20:05:38.688781Z"
    }
   },
   "outputs": [],
   "source": [
    "skplt.metrics.plot_roc(y_test, probabilities, plot_micro=True, plot_macro=True, \n",
    "                       title_fontsize=20, text_fontsize=16, figsize=(8,8), cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:06:02.965386Z",
     "start_time": "2020-06-02T20:06:02.932868Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Area under ROC curve (ROC-AUC):')\n",
    "for i, class_ in enumerate(lreg_final.classes_):\n",
    "    print('Class {}:'.format(class_), round(roc_auc_score(y_bin[:,i],lreg_final.predict_proba(X_test)[:,i]),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T20:06:14.619850Z",
     "start_time": "2020-06-02T20:06:14.616426Z"
    }
   },
   "outputs": [],
   "source": [
    "# skplt.metrics.roc_curve(y_test==0, probabilities[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textacy",
   "language": "python",
   "name": "textacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
