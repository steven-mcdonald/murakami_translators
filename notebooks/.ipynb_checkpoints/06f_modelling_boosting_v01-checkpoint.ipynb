{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> Main Modelling - Boosting - Gridsearch </span>\n",
    "* read in pickle\n",
    "* keep 3 translators\n",
    "* final selected features based on initial analysis with logistic regression\n",
    "* drop 'A Wild Sheep Chase' due to unique text\n",
    "* run boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:12:43.762915Z",
     "start_time": "2020-05-16T21:12:39.076667Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/Steven/opt/anaconda3/envs/textacy/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textacy\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, average_precision_score, roc_auc_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "# extend limit of number of rows and columns to display in cell\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataframe containing text chunks and related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:13:09.956080Z",
     "start_time": "2020-05-16T21:12:43.769635Z"
    }
   },
   "outputs": [],
   "source": [
    "in_full_path = '../../../Documents/murakami/pkl3/df_all_v02.pkl'\n",
    "# read back pickle\n",
    "with open (in_full_path, 'rb') as fp:\n",
    "    df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the dataframe is as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:13:10.057310Z",
     "start_time": "2020-05-16T21:13:09.961087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_chunk_no</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>fstop_indices</th>\n",
       "      <th>split_indices</th>\n",
       "      <th>chunks</th>\n",
       "      <th>translator</th>\n",
       "      <th>book_title</th>\n",
       "      <th>parsed</th>\n",
       "      <th>n_sents</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>n_syllables</th>\n",
       "      <th>n_unique_words</th>\n",
       "      <th>n_long_words</th>\n",
       "      <th>n_monosyllable_words</th>\n",
       "      <th>n_polysyllable_words</th>\n",
       "      <th>trans_code</th>\n",
       "      <th>chunk_length</th>\n",
       "      <th>n_sents_norm</th>\n",
       "      <th>n_words_norm</th>\n",
       "      <th>n_chars_norm</th>\n",
       "      <th>n_syllables_norm</th>\n",
       "      <th>n_unique_words_norm</th>\n",
       "      <th>n_long_words_norm</th>\n",
       "      <th>n_monosyllable_words_norm</th>\n",
       "      <th>n_polysyllable_words_norm</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>pron_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>det_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>num_count</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adp_count</th>\n",
       "      <th>cconj_count</th>\n",
       "      <th>sconj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>aux_count</th>\n",
       "      <th>part_count</th>\n",
       "      <th>propn_count</th>\n",
       "      <th>space_count</th>\n",
       "      <th>intj_count</th>\n",
       "      <th>sym_count</th>\n",
       "      <th>x_count</th>\n",
       "      <th>...</th>\n",
       "      <th>where_adv</th>\n",
       "      <th>too_adv</th>\n",
       "      <th>now_adv</th>\n",
       "      <th>all_adv</th>\n",
       "      <th>then_adv</th>\n",
       "      <th>just_adv</th>\n",
       "      <th>really_adv</th>\n",
       "      <th>there_adv</th>\n",
       "      <th>only_adv</th>\n",
       "      <th>even_adv</th>\n",
       "      <th>as_adv</th>\n",
       "      <th>always_adv</th>\n",
       "      <th>when_adv</th>\n",
       "      <th>again_adv</th>\n",
       "      <th>here_adv</th>\n",
       "      <th>maybe_adv</th>\n",
       "      <th>still_adv</th>\n",
       "      <th>back_adv</th>\n",
       "      <th>why_adv</th>\n",
       "      <th>very_adv</th>\n",
       "      <th>never_adv</th>\n",
       "      <th>how_adv</th>\n",
       "      <th>whole_adj</th>\n",
       "      <th>small_adj</th>\n",
       "      <th>long_adj</th>\n",
       "      <th>able_adj</th>\n",
       "      <th>more_adj</th>\n",
       "      <th>sure_adj</th>\n",
       "      <th>big_adj</th>\n",
       "      <th>last_adj</th>\n",
       "      <th>bad_adj</th>\n",
       "      <th>old_adj</th>\n",
       "      <th>same_adj</th>\n",
       "      <th>good_adj</th>\n",
       "      <th>new_adj</th>\n",
       "      <th>only_adj</th>\n",
       "      <th>different_adj</th>\n",
       "      <th>other_adj</th>\n",
       "      <th>next_adj</th>\n",
       "      <th>few_adj</th>\n",
       "      <th>much_adj</th>\n",
       "      <th>hard_adj</th>\n",
       "      <th>real_adj</th>\n",
       "      <th>young_adj</th>\n",
       "      <th>own_adj</th>\n",
       "      <th>first_adj</th>\n",
       "      <th>deep_adj</th>\n",
       "      <th>strange_adj</th>\n",
       "      <th>little_adj</th>\n",
       "      <th>right_adj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wednesday Afternoon Picnic</td>\n",
       "      <td>IT WAS A  short one-paragraph item in the morn...</td>\n",
       "      <td>[57, 97, 115, 196, 318, 385, 420, 445, 504, 65...</td>\n",
       "      <td>[967, 1924, 2998, 3982, 4935, 5975, 6995, 7961...</td>\n",
       "      <td>IT WAS A short one-paragraph item in the morni...</td>\n",
       "      <td>Alfred Birnbaum</td>\n",
       "      <td>A Wild Sheep Chase</td>\n",
       "      <td>(IT, WAS, A, short, one, -, paragraph, item, i...</td>\n",
       "      <td>15</td>\n",
       "      <td>174</td>\n",
       "      <td>742</td>\n",
       "      <td>240</td>\n",
       "      <td>116</td>\n",
       "      <td>33</td>\n",
       "      <td>128</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>944</td>\n",
       "      <td>15.889831</td>\n",
       "      <td>184.322034</td>\n",
       "      <td>786.016949</td>\n",
       "      <td>254.237288</td>\n",
       "      <td>122.881356</td>\n",
       "      <td>34.957627</td>\n",
       "      <td>135.59322</td>\n",
       "      <td>18.008475</td>\n",
       "      <td>-0.4798</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.064</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_chunk_no number                       title  \\\n",
       "0              0      1  Wednesday Afternoon Picnic   \n",
       "\n",
       "                                                text  \\\n",
       "0  IT WAS A  short one-paragraph item in the morn...   \n",
       "\n",
       "                                       fstop_indices  \\\n",
       "0  [57, 97, 115, 196, 318, 385, 420, 445, 504, 65...   \n",
       "\n",
       "                                       split_indices  \\\n",
       "0  [967, 1924, 2998, 3982, 4935, 5975, 6995, 7961...   \n",
       "\n",
       "                                              chunks       translator  \\\n",
       "0  IT WAS A short one-paragraph item in the morni...  Alfred Birnbaum   \n",
       "\n",
       "           book_title                                             parsed  \\\n",
       "0  A Wild Sheep Chase  (IT, WAS, A, short, one, -, paragraph, item, i...   \n",
       "\n",
       "   n_sents  n_words  n_chars  n_syllables  n_unique_words  n_long_words  \\\n",
       "0       15      174      742          240             116            33   \n",
       "\n",
       "   n_monosyllable_words  n_polysyllable_words  trans_code  chunk_length  \\\n",
       "0                   128                    17           0           944   \n",
       "\n",
       "   n_sents_norm  n_words_norm  n_chars_norm  n_syllables_norm  \\\n",
       "0     15.889831    184.322034    786.016949        254.237288   \n",
       "\n",
       "   n_unique_words_norm  n_long_words_norm  n_monosyllable_words_norm  \\\n",
       "0           122.881356          34.957627                  135.59322   \n",
       "\n",
       "   n_polysyllable_words_norm  vader_compound  vader_neg  vader_neu  vader_pos  \\\n",
       "0                  18.008475         -0.4798      0.075      0.862      0.064   \n",
       "\n",
       "   pron_count  verb_count  det_count  adj_count  num_count  punct_count  \\\n",
       "0        18.0        20.0       31.0        9.0        2.0         33.0   \n",
       "\n",
       "   noun_count  adp_count  cconj_count  sconj_count  adv_count  aux_count  \\\n",
       "0        51.0       19.0          6.0          3.0        5.0        5.0   \n",
       "\n",
       "   part_count  propn_count  space_count  intj_count  sym_count  x_count  ...  \\\n",
       "0         3.0          2.0          0.0         0.0        0.0      0.0  ...   \n",
       "\n",
       "   where_adv  too_adv  now_adv  all_adv  then_adv  just_adv  really_adv  \\\n",
       "0          1        0        0        0         0         0           0   \n",
       "\n",
       "   there_adv  only_adv  even_adv  as_adv  always_adv  when_adv  again_adv  \\\n",
       "0          0         0         1       0           0         0          0   \n",
       "\n",
       "   here_adv  maybe_adv  still_adv  back_adv  why_adv  very_adv  never_adv  \\\n",
       "0         0          0          0         0        0         0          0   \n",
       "\n",
       "   how_adv  whole_adj  small_adj  long_adj  able_adj  more_adj  sure_adj  \\\n",
       "0        0          1          0         0         0         0         0   \n",
       "\n",
       "   big_adj  last_adj  bad_adj  old_adj  same_adj  good_adj  new_adj only_adj  \\\n",
       "0        0         0        0        1         0         0        0        0   \n",
       "\n",
       "  different_adj other_adj next_adj  few_adj  much_adj  hard_adj  real_adj  \\\n",
       "0             0         0        0        0         0         0         0   \n",
       "\n",
       "   young_adj  own_adj  first_adj  deep_adj  strange_adj  little_adj  right_adj  \n",
       "0          0        0          0         0            0           0          0  \n",
       "\n",
       "[1 rows x 141 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:13:14.335322Z",
     "start_time": "2020-05-16T21:13:14.327819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5253, 141)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T14:14:06.831712Z",
     "start_time": "2020-05-10T14:14:06.826251Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop 'A Wild Sheep Chase' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:13:18.028113Z",
     "start_time": "2020-05-16T21:13:17.970532Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['book_title'] != 'A Wild Sheep Chase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:13:21.656565Z",
     "start_time": "2020-05-16T21:13:21.642934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4364282726892514"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_acc = df['trans_code'].value_counts(normalize=True).max()\n",
    "baseline_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Predictor Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:13:25.788199Z",
     "start_time": "2020-05-16T21:13:25.769058Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_select(basic_counts=True, vader=False, pos_counts=False, \n",
    "                   words=False, adv=False, adj=False):\n",
    "    '''create column list depending on features to include in the modelling'''\n",
    "    columns = []\n",
    "    if basic_counts:\n",
    "        columns += [i for i in df.columns if i.startswith('n_') & i.endswith('_norm')]\n",
    "    if vader:\n",
    "        columns += [i for i in df.columns if i.startswith('vader_')]\n",
    "    if pos_counts:\n",
    "        columns += [i for i in df.columns if i.endswith('_count_norm')]\n",
    "    if words:\n",
    "        columns += [i for i in df.columns if i.endswith('_w')]\n",
    "    if adj:\n",
    "        columns += [i for i in df.columns if i.endswith('_adj')]\n",
    "    if adv:\n",
    "        columns += [i for i in df.columns if i.endswith('_adv')]\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:13:29.218034Z",
     "start_time": "2020-05-16T21:13:29.212647Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = ['num_count_norm', 'punct_count_norm','space_count_norm', 'sym_count_norm', 'x_count_norm']\n",
    "predictor_cols = feature_select(basic_counts=True, pos_counts=True, vader=False, words=True, adj=True, adv=True)\n",
    "predictor_cols = [x for x in predictor_cols if x not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Target and Predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:13:37.387104Z",
     "start_time": "2020-05-16T21:13:37.380225Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df[predictor_cols]\n",
    "y = df['trans_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Train Test Split with stratify on y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:13:44.405883Z",
     "start_time": "2020-05-16T21:13:44.389675Z"
    }
   },
   "outputs": [],
   "source": [
    "# stratify based on your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise the predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:14:01.095562Z",
     "start_time": "2020-05-16T21:14:01.072120Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboosting classification with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:25:29.949146Z",
     "start_time": "2020-05-16T21:22:47.093399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=2)]: Done 240 out of 240 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                          base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                                                class_weight=None,\n",
       "                                                                                criterion='gini',\n",
       "                                                                                max_depth=3,\n",
       "                                                                                max_features='auto',\n",
       "                                                                                max_leaf_nodes=None,\n",
       "                                                                                min_impurity_decrease=0.0,\n",
       "                                                                                min_impurity_split=None,\n",
       "                                                                                min_samples_leaf=1,\n",
       "                                                                                min_samples_split=2,\n",
       "                                                                                min_weight_fraction_leaf=0.0,\n",
       "                                                                                presort='deprecated',\n",
       "                                                                                random_state=None,\n",
       "                                                                                splitter='best'),\n",
       "                                          learning_rate=1.0, n_estimators=100,\n",
       "                                          random_state=1),\n",
       "             iid='deprecated', n_jobs=2,\n",
       "             param_grid={'base_estimator__max_depth': [2, 3, 4, 5],\n",
       "                         'learning_rate': [0.001, 0.01, 0.1, 1],\n",
       "                         'n_estimators': [100, 200, 300]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=3, max_features = \"auto\")\n",
    "model = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=1)\n",
    "# params = {'max_samples': np.linspace(0.8, 1.0, 3),\n",
    "#           'max_features': range(int(3/4.*X.shape[1]), X.shape[1]+1)}\n",
    "# param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "#               \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "#               \"n_estimators\": [100, 200]\n",
    "#              }\n",
    "params = {'n_estimators': [100, 200, 300],\n",
    "          'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "          'base_estimator__max_depth' : [2,3,4,5]}\n",
    "\n",
    "bs_dt_gs_01 = GridSearchCV(model, \n",
    "                           param_grid=params, \n",
    "                           cv=5, \n",
    "                           n_jobs=2,\n",
    "                           verbose=1)\n",
    "\n",
    "bs_dt_gs_01.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:25:52.738875Z",
     "start_time": "2020-05-16T21:25:52.728672Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_gridsearch_score(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # print the grid search results\n",
    "    results_dict = {}\n",
    "    print('Best Parameters:')\n",
    "    print(model.best_params_)\n",
    "    print('Best estimator mean cross validated training score:')\n",
    "    print(model.best_score_)\n",
    "    print('Best estimator score on the full training set:')\n",
    "    print(model.score(X_train, y_train))\n",
    "    print('Best estimator score on the test set:')\n",
    "    print(model.score(X_test, y_test))\n",
    "    print('ROC-AUC score on the test set:')\n",
    "    \n",
    "    # store the grid search results as a dictionary\n",
    "    y_bin = label_binarize(y_test, model.classes_)\n",
    "    for i, class_ in enumerate(model.classes_):\n",
    "        print('Class {}:'.format(class_), round(roc_auc_score(y_bin[:,i],model.predict_proba(X_test)[:,i]),2))\n",
    "    results_dict['Best_Parameters'] = model.best_params_\n",
    "    results_dict['Best_CV_Score'] = model.best_score_\n",
    "    results_dict['Best_Train_Score'] = model.score(X_train, y_train)\n",
    "    results_dict['Best_Test_Score'] = model.score(X_test, y_test)\n",
    "    results_dict['AUC_Class_0'] = roc_auc_score(y_bin[:,0],model.predict_proba(X_test)[:,0])\n",
    "    results_dict['AUC_Class_1'] = roc_auc_score(y_bin[:,1],model.predict_proba(X_test)[:,1])\n",
    "    results_dict['AUC_Class_2'] = roc_auc_score(y_bin[:,2],model.predict_proba(X_test)[:,2])\n",
    "    predictions = model.predict(X_test)\n",
    "    results_dict['conmat'] = confusion_matrix(\n",
    "        y_test, predictions, labels=[0, 1, 2])\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:25:56.150596Z",
     "start_time": "2020-05-16T21:25:54.380982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'base_estimator__max_depth': 2, 'learning_rate': 0.1, 'n_estimators': 300}\n",
      "Best estimator mean cross validated training score:\n",
      "0.6543790849673201\n",
      "Best estimator score on the full training set:\n",
      "0.7566013071895424\n",
      "Best estimator score on the test set:\n",
      "0.6750261233019854\n",
      "ROC-AUC score on the test set:\n",
      "Class 0: 0.8\n",
      "Class 1: 0.83\n",
      "Class 2: 0.79\n"
     ]
    }
   ],
   "source": [
    "bs_dt_gs_01_results = model_gridsearch_score(bs_dt_gs_01, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:40:47.375076Z",
     "start_time": "2020-05-16T21:40:47.356134Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the output path and name\n",
    "out_path = '../../../Documents/murakami/pkl_models/'\n",
    "out_name = 'bs_dt_gs_01_ns'\n",
    "out_full_path = out_path + out_name + '.pkl'\n",
    "\n",
    "# save pickle\n",
    "with open(out_full_path, 'wb') as fp:\n",
    "    pickle.dump(bs_dt_gs_01, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ada boost with dt is not great for the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:27:29.824454Z",
     "start_time": "2020-05-16T21:27:29.685133Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = bs_dt_gs_01.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:27:32.872015Z",
     "start_time": "2020-05-16T21:27:32.857937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted birnbaum</th>\n",
       "      <th>predicted rubin</th>\n",
       "      <th>predicted gabriel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual birnbaum</th>\n",
       "      <td>144</td>\n",
       "      <td>73</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual rubin</th>\n",
       "      <td>43</td>\n",
       "      <td>329</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual gabriel</th>\n",
       "      <td>37</td>\n",
       "      <td>71</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 predicted birnbaum  predicted rubin  predicted gabriel\n",
       "actual birnbaum                 144               73                 41\n",
       "actual rubin                     43              329                 46\n",
       "actual gabriel                   37               71                173"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conmat = confusion_matrix(\n",
    "    y_test, predictions, labels=[0, 1, 2])\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=['actual birnbaum', 'actual rubin', 'actual gabriel'],\n",
    "                         columns=['predicted birnbaum', 'predicted rubin', 'predicted gabriel'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OK results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:27:40.311815Z",
     "start_time": "2020-05-16T21:27:40.299240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.56      0.60       258\n",
      "           1       0.70      0.79      0.74       418\n",
      "           2       0.67      0.62      0.64       281\n",
      "\n",
      "    accuracy                           0.68       957\n",
      "   macro avg       0.67      0.65      0.66       957\n",
      "weighted avg       0.67      0.68      0.67       957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:35:04.035341Z",
     "start_time": "2020-05-16T21:33:44.371304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=1, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_01 = GradientBoostingClassifier(\n",
    "    n_estimators=1000,\n",
    "    criterion='mse',\n",
    "    loss='deviance',\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=1)\n",
    "\n",
    "gb_01.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:40:31.311882Z",
     "start_time": "2020-05-16T21:35:19.320426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n",
      "CV score: 0.6847058823529413\n",
      "test score: 0.7136886102403344\n"
     ]
    }
   ],
   "source": [
    "print('train score:', gb_01.score(X_train, y_train))\n",
    "print('CV score:', cross_val_score(gb_01, X_train, y_train, cv=5).mean())\n",
    "print('test score:', gb_01.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:41:44.771815Z",
     "start_time": "2020-05-16T21:41:44.677236Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the output path and name\n",
    "out_path = '../../../Documents/murakami/pkl_models/'\n",
    "out_name = 'gb_01_ns'\n",
    "out_full_path = out_path + out_name + '.pkl'\n",
    "\n",
    "# save pickle\n",
    "with open(out_full_path, 'wb') as fp:\n",
    "    pickle.dump(gb_01, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:41:49.595386Z",
     "start_time": "2020-05-16T21:41:49.546454Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = gb_01.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:41:50.502046Z",
     "start_time": "2020-05-16T21:41:50.487879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted birnbaum</th>\n",
       "      <th>predicted rubin</th>\n",
       "      <th>predicted gabriel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual birnbaum</th>\n",
       "      <td>150</td>\n",
       "      <td>61</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual rubin</th>\n",
       "      <td>47</td>\n",
       "      <td>338</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual gabriel</th>\n",
       "      <td>40</td>\n",
       "      <td>46</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 predicted birnbaum  predicted rubin  predicted gabriel\n",
       "actual birnbaum                 150               61                 47\n",
       "actual rubin                     47              338                 33\n",
       "actual gabriel                   40               46                195"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conmat = confusion_matrix(\n",
    "    y_test, predictions, labels=[0, 1, 2])\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=['actual birnbaum', 'actual rubin', 'actual gabriel'],\n",
    "                         columns=['predicted birnbaum', 'predicted rubin', 'predicted gabriel'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T21:42:02.755334Z",
     "start_time": "2020-05-16T21:42:02.743875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.58      0.61       258\n",
      "           1       0.76      0.81      0.78       418\n",
      "           2       0.71      0.69      0.70       281\n",
      "\n",
      "    accuracy                           0.71       957\n",
      "   macro avg       0.70      0.69      0.70       957\n",
      "weighted avg       0.71      0.71      0.71       957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try grid search for Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T06:34:19.980009Z",
     "start_time": "2020-05-17T06:18:11.354373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=2)]: Done 180 out of 180 | elapsed: 15.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=GradientBoostingClassifier(ccp_alpha=0.0,\n",
       "                                                  criterion='friedman_mse',\n",
       "                                                  init=None, learning_rate=0.1,\n",
       "                                                  loss='deviance', max_depth=3,\n",
       "                                                  max_features=None,\n",
       "                                                  max_leaf_nodes=None,\n",
       "                                                  min_impurity_decrease=0.0,\n",
       "                                                  min_impurity_split=None,\n",
       "                                                  min_samples_leaf=1,\n",
       "                                                  min_samples_split=2,\n",
       "                                                  min_weight_fraction_leaf=0.0,\n",
       "                                                  n_estimators=100,\n",
       "                                                  n_iter_no_c...\n",
       "                                                  presort='deprecated',\n",
       "                                                  random_state=None,\n",
       "                                                  subsample=1.0, tol=0.0001,\n",
       "                                                  validation_fraction=0.1,\n",
       "                                                  verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=2,\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 0.2],\n",
       "                         'loss': ['deviance'], 'max_depth': [3, 5],\n",
       "                         'n_estimators': [100, 200],\n",
       "                         'subsample': [0.5, 0.75, 1.0]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "#     \"min_samples_split\": [0.1, 0.25, 0.5],\n",
    "#     \"min_samples_leaf\": [0.1, 0.25, 0.5],\n",
    "    \"max_depth\":[3,5],\n",
    "#     \"max_features\":[\"log2\",\"sqrt\"],\n",
    "#     \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"subsample\":[0.5, 0.75, 1.0],\n",
    "    \"n_estimators\":[100, 200]\n",
    "    }\n",
    "#passing the scoring function in the GridSearchCV\n",
    "gb_gs_01 = GridSearchCV(GradientBoostingClassifier(),\n",
    "                        parameters,\n",
    "                        refit=True,\n",
    "                        cv=5, \n",
    "                        n_jobs=2,\n",
    "                        verbose=1)\n",
    "\n",
    "gb_gs_01.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T06:35:42.319337Z",
     "start_time": "2020-05-17T06:35:42.016947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.5}\n",
      "Best estimator mean cross validated training score:\n",
      "0.6951633986928105\n",
      "Best estimator score on the full training set:\n",
      "0.9997385620915032\n",
      "Best estimator score on the test set:\n",
      "0.7115987460815048\n",
      "ROC-AUC score on the test set:\n",
      "Class 0: 0.84\n",
      "Class 1: 0.88\n",
      "Class 2: 0.88\n"
     ]
    }
   ],
   "source": [
    "gb_gs_01_results = model_gridsearch_score(gb_gs_01, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T06:36:09.748789Z",
     "start_time": "2020-05-17T06:36:09.737399Z"
    }
   },
   "outputs": [],
   "source": [
    "best_params = gb_gs_01.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T06:36:10.213477Z",
     "start_time": "2020-05-17T06:36:10.193579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 5,\n",
       " 'n_estimators': 200,\n",
       " 'subsample': 0.5}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T06:38:25.142900Z",
     "start_time": "2020-05-17T06:38:25.089566Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the output path and name\n",
    "out_path = '../../../Documents/murakami/pkl_models/'\n",
    "out_name = 'gb_gs_01_ns'\n",
    "out_full_path = out_path + out_name + '.pkl'\n",
    "\n",
    "# save pickle\n",
    "with open(out_full_path, 'wb') as fp:\n",
    "    pickle.dump(gb_gs_01, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T07:29:02.073517Z",
     "start_time": "2020-05-17T07:29:02.023454Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = gb_gs_01.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T07:29:02.828464Z",
     "start_time": "2020-05-17T07:29:02.807478Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted birnbaum</th>\n",
       "      <th>predicted rubin</th>\n",
       "      <th>predicted gabriel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual birnbaum</th>\n",
       "      <td>150</td>\n",
       "      <td>67</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual rubin</th>\n",
       "      <td>40</td>\n",
       "      <td>341</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual gabriel</th>\n",
       "      <td>35</td>\n",
       "      <td>56</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 predicted birnbaum  predicted rubin  predicted gabriel\n",
       "actual birnbaum                 150               67                 41\n",
       "actual rubin                     40              341                 37\n",
       "actual gabriel                   35               56                190"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conmat = confusion_matrix(\n",
    "    y_test, predictions, labels=[0, 1, 2])\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=['actual birnbaum', 'actual rubin', 'actual gabriel'],\n",
    "                         columns=['predicted birnbaum', 'predicted rubin', 'predicted gabriel'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T07:29:05.141553Z",
     "start_time": "2020-05-17T07:29:05.127700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.58      0.62       258\n",
      "           1       0.73      0.82      0.77       418\n",
      "           2       0.71      0.68      0.69       281\n",
      "\n",
      "    accuracy                           0.71       957\n",
      "   macro avg       0.70      0.69      0.70       957\n",
      "weighted avg       0.71      0.71      0.71       957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textacy",
   "language": "python",
   "name": "textacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
